{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook contains codes to run hyper-parameter tuning using a genetic algorithm.\n",
    "Use another notebook if you wish to use *grid search* instead.\n",
    "# Under development.\n",
    "\"\"\"\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "# If this notebook file is not placed under in /notebook/ directory,\n",
    "# adding directory \"../\" might not correly add the project directory.\n",
    "# If adding \"../\" does not solve the importing problem, we need to setup \n",
    "# the directory mannually.\n",
    "try:\n",
    "    import constants\n",
    "except ModuleNotFoundError:\n",
    "    core_dir = input(\"Directory of core files >>> \")\n",
    "    if not core_dir.endswith(\"/\"):\n",
    "        core_dir += \"/\"\n",
    "    sys.path.append(core_dir)\n",
    "    import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from core.tools.metrics import *\n",
    "import core.tools.visualize as visualize\n",
    "from core.tools.time_series import *\n",
    "from core.tools.data_import import *\n",
    "import core.tools.rnn_prepare as rnn_prepare\n",
    "import core.tools.param_set_generator as param_set_generator\n",
    "import core.ga.genetic_hpt as genetic_hpt\n",
    "\n",
    "import core.models.stacked_lstm as stacked_lstm\n",
    "\n",
    "import core.training.hps_methods as hps_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': '/Users/tianyudu/Documents/Academics/EconForecasting/AnnEconForecast/data/UNRATE.csv',\n",
      " 'b': '/home/ec2-user/AnnEconForecast/data/UNRATE.csv',\n",
      " 'c': '/home/ec2-user/AnnEconForecast/data/DEXCAUS.csv'}\n",
      "Select Dataset >>> b\n",
      "Dataset chosen: /home/ec2-user/AnnEconForecast/data/UNRATE.csv\n",
      "Avaiable configuration files found: \n",
      "\tec2_config.py\n",
      "\tmac_config.py\n",
      "Select config file >>> ec2_config\n"
     ]
    }
   ],
   "source": [
    "# data preparation phase.\n",
    "pprint(constants.DATA_DIR)\n",
    "choice = None\n",
    "while choice is None or choice not in constants.DATA_DIR.keys():\n",
    "    if choice is not None:\n",
    "        print(\"Invalid data location received, try again...\")\n",
    "    choice = input(\"Select Dataset >>> \")\n",
    "# choice = \"a\"\n",
    "\n",
    "FILE_DIR = constants.DATA_DIR[choice]\n",
    "\n",
    "print(f\"Dataset chosen: {FILE_DIR}\")\n",
    "\n",
    "print(\"Avaiable configuration files found: \")\n",
    "for cf in os.listdir(\"../hps_configs\"):\n",
    "    if cf.endswith(\"config.py\"):\n",
    "        print(\"\\t\" + cf)\n",
    "\n",
    "config_name = input(\"Select config file >>> \")\n",
    "if config_name.endswith(\".py\"):\n",
    "    config_name = config_name[:-3]\n",
    "# config_name = \"mac_config\"\n",
    "\n",
    "exec(f\"import hps_configs.{config_name} as config\")\n",
    "\n",
    "# print(\"Reading configuration file...\")\n",
    "# for att in dir(config):\n",
    "#     if att.endswith(\"_config\"):\n",
    "#         print(f\"\\tLoading: {att}\")\n",
    "#         exec(f\"globals().update(config.{att})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def obj_func(param) -> float:\n",
    "    df_ready = rnn_prepare.prepare_dataset(\n",
    "    file_dir=FILE_DIR,\n",
    "    periods=int(param[\"PERIODS\"]),\n",
    "    order=int(param[\"ORDER\"]),\n",
    "    remove=None,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "    # Split dataset.\n",
    "    (X_train, X_val, X_test,\n",
    "     y_train, y_val, y_test) = rnn_prepare.split_dataset(\n",
    "        raw=df_ready,\n",
    "        train_ratio=param[\"TRAIN_RATIO\"],\n",
    "        val_ratio=param[\"VAL_RATIO\"],\n",
    "        lags=param[\"LAGS\"]\n",
    "    )\n",
    "\n",
    "    # The gross dataset excluding the test set.\n",
    "    # Excluding the test set for isolation purpose.\n",
    "    data_feed = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "    }\n",
    "    ep = param[\"epochs\"]\n",
    "    ckpts = range(int(ep * 0.95), ep)  # Take the final 5% epochs.\n",
    "    tf.reset_default_graph()\n",
    "    model = stacked_lstm.StackedLSTM(\n",
    "    param=param,\n",
    "    prediction_checkpoints=ckpts,\n",
    "    verbose=False\n",
    "    )\n",
    "    \n",
    "    ret_pack = model.fit(data=data_feed, ret=[\"mse_val\"])\n",
    "    return float(np.mean(list(ret_pack[\"mse_val\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gen = 5\n",
    "init_size = 5\n",
    "ignore_set = (\n",
    "    \"PERIODS\", \"ORDER\", \"TRAIN_RATIO\", \"VAL_RATIO\", \"num_outputs\", \"num_inputs\", \"report_periods\",\n",
    "    \"tensorboard_path\", \"model_path\", \"fig_path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = genetic_hpt.GeneticHPT(\n",
    "    gene_pool=config.main,\n",
    "    pop_size=init_size,\n",
    "    eval_func=obj_func,\n",
    "    mode=\"min\",\n",
    "    retain=0.5,\n",
    "    shot_prob=0.05,\n",
    "    mutate_prob=0.05,\n",
    "    verbose=False,\n",
    "    ignore=ignore_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_param = {'LAGS': 6,\n",
    " 'ORDER': 1,\n",
    " 'PERIODS': 1,\n",
    " 'TRAIN_RATIO': 0.8,\n",
    " 'VAL_RATIO': 0.1,\n",
    " 'clip_grad': None,\n",
    " 'epochs': 500,\n",
    " 'fig_path': '/Volumes/Intel/debug/model_figs/',\n",
    " 'learning_rate': 0.1,\n",
    " 'model_path': '/Volumes/Intel/debug/saved_models/',\n",
    " 'num_inputs': 1,\n",
    " 'num_neurons': (32, 64),\n",
    " 'num_outputs': 1,\n",
    " 'num_time_steps': None,\n",
    " 'report_periods': 10,\n",
    " 'tensorboard_path': '/Volumes/Intel/debug/tensorboard/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LAGS': 11,\n",
       " 'ORDER': 1,\n",
       " 'PERIODS': 1,\n",
       " 'TRAIN_RATIO': 0.8,\n",
       " 'VAL_RATIO': 0.1,\n",
       " 'clip_grad': None,\n",
       " 'epochs': 190,\n",
       " 'fig_path': '/Volumes/Intel/debug/model_figs/',\n",
       " 'learning_rate': 1.40477971486475,\n",
       " 'model_path': '/Volumes/Intel/debug/saved_models/',\n",
       " 'num_inputs': 1,\n",
       " 'num_neurons': (99, 70),\n",
       " 'num_outputs': 1,\n",
       " 'num_time_steps': None,\n",
       " 'report_periods': 10,\n",
       " 'tensorboard_path': '/Volumes/Intel/debug/tensorboard/'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.mutate(sample_param, mutate_prob=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation gen=0...\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"Initial evaluation gen=0...\")\n",
    "optimizer.evaluate(verbose=True)\n",
    "print(f\"\\nBest fitted entity validation MSE: {optimizer.population[0][1]: 0.7f}\\\n",
    "\\nWorst fitted entity validation MSE: {optimizer.population[-1][1]: 0.7f}\")\n",
    "for gen in range(total_gen):\n",
    "    print(f\"Generation: [{gen + 1}/{total_gen}]\")\n",
    "    optimizer.select()\n",
    "    optimizer.evolve()\n",
    "    optimizer.evaluate(verbose=True)\n",
    "    print(f\"\\nBest fitted entity validation MSE: {optimizer.population[0][1]: 0.7f}\\\n",
    "    \\nWorst fitted entity validation MSE: {optimizer.population[-1][1]: 0.7f}\")\n",
    "print(f\"Final generation best fitted entity: {optimizer.population[0][0]}\\\n",
    "\\nwith valudation set MSE (fitness): {optimizer.population[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for (i, param) in enumerate(parameter_collection):\n",
    "#     visualize.progbar(i, len(parameter_collection), 80)\n",
    "#     with HiddenPrints():\n",
    "#         print(\"================================================================\")\n",
    "#         print(f\"Executing hyper-parameter searching session [{i}/{len(parameter_collection) - 1}]...\")\n",
    "#         print(\"Session Flexiable Config:\\n\\t\" + param[\"hparam_str\"].replace(\"-\", \"\\n\\t\"))\n",
    "#         start = datetime.now()\n",
    "#         hps_methods.individual_train(\n",
    "#             param=param,\n",
    "#             exec_core=stacked_lstm.exec_core,\n",
    "#             file_dir=FILE_DIR\n",
    "#         )\n",
    "#         print(f\"Time taken for session [{i}]: {str(datetime.now() - start)}.\")\n",
    "# print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
