{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook is used for model hyper-parameter searching.\n",
    "Also, if this can also be used as a baseline training script.\n",
    "\"\"\"\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "# If this notebook file is not placed under in /notebook/ directory,\n",
    "# adding directory \"../\" might not correly add the project directory.\n",
    "# If adding \"../\" does not solve the importing problem, we need to setup \n",
    "# the directory mannually.\n",
    "try:\n",
    "    import constants\n",
    "except ModuleNotFoundError:\n",
    "    core_dir = input(\"Directory of core files >>> \")\n",
    "    if not core_dir.endswith(\"/\"):\n",
    "        core_dir += \"/\"\n",
    "    sys.path.append(core_dir)\n",
    "    import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from core.tools.metrics import *\n",
    "import core.tools.visualize as visualize\n",
    "from core.tools.time_series import *\n",
    "from core.tools.data_import import *\n",
    "import core.tools.rnn_prepare as rnn_prepare\n",
    "import core.tools.param_set_generator as param_set_generator\n",
    "\n",
    "import core.models.stacked_lstm as stacked_lstm\n",
    "\n",
    "import core.training.hps_methods as hps_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': '/Users/tianyudu/Documents/Academics/EconForecasting/AnnEconForecast/data/UNRATE.csv',\n",
      " 'b': '/home/ec2-user/AnnEconForecast/data/UNRATE.csv',\n",
      " 'c': '/home/ec2-user/AnnEconForecast/data/DEXCAUS.csv'}\n",
      "Select Dataset >>> b\n",
      "Dataset chosen: /home/ec2-user/AnnEconForecast/data/UNRATE.csv\n",
      "Avaiable configuration files found: \n",
      "\tec2_config.py\n",
      "\tmac_config.py\n",
      "\tsample_config.py\n",
      "Select config file >>> ec2_config\n"
     ]
    }
   ],
   "source": [
    "# data preparation phase.\n",
    "pprint(constants.DATA_DIR)\n",
    "choice = None\n",
    "while choice is None or choice not in constants.DATA_DIR.keys():\n",
    "    if choice is not None:\n",
    "        print(\"Invalid data location received, try again...\")\n",
    "    choice = input(\"Select Dataset >>> \")\n",
    "\n",
    "FILE_DIR = constants.DATA_DIR[choice]\n",
    "\n",
    "print(f\"Dataset chosen: {FILE_DIR}\")\n",
    "\n",
    "print(\"Avaiable configuration files found: \")\n",
    "for cf in os.listdir(\"../hps_configs\"):\n",
    "    if cf.endswith(\"config.py\"):\n",
    "        print(\"\\t\" + cf)\n",
    "\n",
    "config_name = input(\"Select config file >>> \")\n",
    "if config_name.endswith(\".py\"):\n",
    "    config_name = config_name[:-3]\n",
    "\n",
    "exec(f\"import hps_configs.{config_name} as config\")\n",
    "\n",
    "# print(\"Reading configuration file...\")\n",
    "# for att in dir(config):\n",
    "#     if att.endswith(\"_config\"):\n",
    "#         print(f\"\\tLoading: {att}\")\n",
    "#         exec(f\"globals().update(config.{att})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameter sets generated: 864\n"
     ]
    }
   ],
   "source": [
    "parameter_collection = param_set_generator.gen_hparam_set(config.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LAGS': 3,\n",
      " 'ORDER': 1,\n",
      " 'PERIODS': 1,\n",
      " 'TRAIN_RATIO': 0.8,\n",
      " 'VAL_RATIO': 0.1,\n",
      " 'clip_grad': 10,\n",
      " 'epochs': 150,\n",
      " 'fig_path': '/home/ec2-user/ec2_hps/2018DEC21_01/model_figs/LAGS=3-epochs=150-num_neurons=(128, '\n",
      "             '256)-learning_rate=0.003-clip_grad=10',\n",
      " 'hparam_str': 'LAGS=3-epochs=150-num_neurons=(128, '\n",
      "               '256)-learning_rate=0.003-clip_grad=10',\n",
      " 'learning_rate': 0.003,\n",
      " 'model_path': '/home/ec2-user/ec2_hps/2018DEC21_01/saved_models/LAGS=3-epochs=150-num_neurons=(128, '\n",
      "               '256)-learning_rate=0.003-clip_grad=10',\n",
      " 'num_inputs': 1,\n",
      " 'num_neurons': (128, 256),\n",
      " 'num_outputs': 1,\n",
      " 'num_time_steps': None,\n",
      " 'report_periods': 10,\n",
      " 'tensorboard_path': '/home/ec2-user/ec2_hps/2018DEC21_01/tensorboard/LAGS=3-epochs=150-num_neurons=(128, '\n",
      "                     '256)-learning_rate=0.003-clip_grad=10'}\n"
     ]
    }
   ],
   "source": [
    "pprint(parameter_collection[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############################################------------------------------------ [477/864,  55.21%]"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "/home/ec2-user/ec2_hps/2018DEC21_01/saved_models/LAGS=9-epochs=150-num_neurons=(512, 1024, 2048)-learning_rate=0.003-clip_grad=None.data-00000-of-00001.tempstate1239659065685469408; Too many open files\n\t [[node save/SaveV2 (defined at ../core/models/stacked_lstm.py:158)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, OPTIMIZER/beta1_power/_55, OPTIMIZER/beta2_power/_57, OUTPUT_LAYER/BIAS/_59, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER/_61, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER_1/_63, OUTPUT_LAYER/WEIGHT/_65, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER/_67, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER_1/_69, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/_71, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER/_73, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER_1/_75, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/_77, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER/_79, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER_1/_81, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/_83, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER/_85, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER_1/_87, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/_89, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER/_91, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER_1/_93, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/_95, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER/_97, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER_1/_99, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/_101, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER/_103, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER_1/_105)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-6fff477babcc>\", line 11, in <module>\n    file_dir=FILE_DIR\n  File \"../core/training/hps_methods.py\", line 72, in individual_train\n    prediction_checkpoints=ckps\n  File \"../core/models/stacked_lstm.py\", line 158, in exec_core\n    saver = tf.train.Saver()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\n    tensors)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): /home/ec2-user/ec2_hps/2018DEC21_01/saved_models/LAGS=9-epochs=150-num_neurons=(512, 1024, 2048)-learning_rate=0.003-clip_grad=None.data-00000-of-00001.tempstate1239659065685469408; Too many open files\n\t [[node save/SaveV2 (defined at ../core/models/stacked_lstm.py:158)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, OPTIMIZER/beta1_power/_55, OPTIMIZER/beta2_power/_57, OUTPUT_LAYER/BIAS/_59, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER/_61, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER_1/_63, OUTPUT_LAYER/WEIGHT/_65, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER/_67, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER_1/_69, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/_71, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER/_73, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER_1/_75, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/_77, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER/_79, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER_1/_81, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/_83, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER/_85, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER_1/_87, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/_89, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER/_91, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER_1/_93, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/_95, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER/_97, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER_1/_99, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/_101, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER/_103, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER_1/_105)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: /home/ec2-user/ec2_hps/2018DEC21_01/saved_models/LAGS=9-epochs=150-num_neurons=(512, 1024, 2048)-learning_rate=0.003-clip_grad=None.data-00000-of-00001.tempstate1239659065685469408; Too many open files\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, OPTIMIZER/beta1_power/_55, OPTIMIZER/beta2_power/_57, OUTPUT_LAYER/BIAS/_59, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER/_61, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER_1/_63, OUTPUT_LAYER/WEIGHT/_65, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER/_67, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER_1/_69, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/_71, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER/_73, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER_1/_75, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/_77, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER/_79, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER_1/_81, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/_83, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER/_85, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER_1/_87, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/_89, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER/_91, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER_1/_93, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/_95, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER/_97, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER_1/_99, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/_101, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER/_103, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER_1/_105)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6fff477babcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mexec_core\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_core\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mfile_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFILE_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken for session [{i}]: {str(datetime.now() - start)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnnEconForecast/core/training/hps_methods.py\u001b[0m in \u001b[0;36mindividual_train\u001b[0;34m(param, exec_core, file_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_data_feed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mprediction_checkpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnnEconForecast/core/models/stacked_lstm.py\u001b[0m in \u001b[0;36mexec_core\u001b[0;34m(param, data, prediction_checkpoints, verbose)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken for [{param['epochs']}] epochs: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: /home/ec2-user/ec2_hps/2018DEC21_01/saved_models/LAGS=9-epochs=150-num_neurons=(512, 1024, 2048)-learning_rate=0.003-clip_grad=None.data-00000-of-00001.tempstate1239659065685469408; Too many open files\n\t [[node save/SaveV2 (defined at ../core/models/stacked_lstm.py:158)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, OPTIMIZER/beta1_power/_55, OPTIMIZER/beta2_power/_57, OUTPUT_LAYER/BIAS/_59, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER/_61, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER_1/_63, OUTPUT_LAYER/WEIGHT/_65, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER/_67, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER_1/_69, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/_71, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER/_73, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER_1/_75, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/_77, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER/_79, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER_1/_81, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/_83, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER/_85, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER_1/_87, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/_89, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER/_91, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER_1/_93, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/_95, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER/_97, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER_1/_99, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/_101, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER/_103, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER_1/_105)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-6fff477babcc>\", line 11, in <module>\n    file_dir=FILE_DIR\n  File \"../core/training/hps_methods.py\", line 72, in individual_train\n    prediction_checkpoints=ckps\n  File \"../core/models/stacked_lstm.py\", line 158, in exec_core\n    saver = tf.train.Saver()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\n    tensors)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): /home/ec2-user/ec2_hps/2018DEC21_01/saved_models/LAGS=9-epochs=150-num_neurons=(512, 1024, 2048)-learning_rate=0.003-clip_grad=None.data-00000-of-00001.tempstate1239659065685469408; Too many open files\n\t [[node save/SaveV2 (defined at ../core/models/stacked_lstm.py:158)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, OPTIMIZER/beta1_power/_55, OPTIMIZER/beta2_power/_57, OUTPUT_LAYER/BIAS/_59, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER/_61, OUTPUT_LAYER/BIAS/ADAM_OPTIMIZER_1/_63, OUTPUT_LAYER/WEIGHT/_65, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER/_67, OUTPUT_LAYER/WEIGHT/ADAM_OPTIMIZER_1/_69, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/_71, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER/_73, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/bias/ADAM_OPTIMIZER_1/_75, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/_77, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER/_79, rnn/multi_rnn_cell/cell_0/LSTM_CELL_0/kernel/ADAM_OPTIMIZER_1/_81, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/_83, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER/_85, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/bias/ADAM_OPTIMIZER_1/_87, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/_89, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER/_91, rnn/multi_rnn_cell/cell_1/LSTM_CELL_1/kernel/ADAM_OPTIMIZER_1/_93, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/_95, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER/_97, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/bias/ADAM_OPTIMIZER_1/_99, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/_101, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER/_103, rnn/multi_rnn_cell/cell_2/LSTM_CELL_2/kernel/ADAM_OPTIMIZER_1/_105)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "for (i, param) in enumerate(parameter_collection):\n",
    "    visualize.progbar(i, len(parameter_collection), 80)\n",
    "    with HiddenPrints():\n",
    "        print(\"================================================================\")\n",
    "        print(f\"Executing hyper-parameter searching session [{i}/{len(parameter_collection) - 1}]...\")\n",
    "        print(\"Session Flexiable Config:\\n\\t\" + param[\"hparam_str\"].replace(\"-\", \"\\n\\t\"))\n",
    "        start = datetime.now()\n",
    "        hps_methods.individual_train(\n",
    "            param=param,\n",
    "            exec_core=stacked_lstm.exec_core,\n",
    "            file_dir=FILE_DIR\n",
    "        )\n",
    "        print(f\"Time taken for session [{i}]: {str(datetime.now() - start)}.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
