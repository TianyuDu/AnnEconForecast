{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook is used for model hyper-parameter searching.\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "# If this notebook file is not placed under in /notebook/ directory,\n",
    "# adding directory \"../\" might not correly add the project directory.\n",
    "# If adding \"../\" does not solve the importing problem, we need to setup \n",
    "# the directory mannually.\n",
    "try:\n",
    "    import constants\n",
    "except ModuleNotFoundError:\n",
    "    core_dir = input(\"Directory of core files >>> \")\n",
    "    if not core_dir.endswith(\"/\"):\n",
    "        core_dir += \"/\"\n",
    "    sys.path.append(core_dir)\n",
    "    import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tools.metrics import *\n",
    "import core.tools.visualize as visualize\n",
    "from core.tools.time_series import *\n",
    "from core.tools.data_import import *\n",
    "import core.tools.rnn_prepare as rnn_prepare\n",
    "\n",
    "import core.models.stacked_lstm as stacked_lstm\n",
    "\n",
    "import core.training.hps_methods as hps_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': '/Users/tianyudu/Documents/Academics/EconForecasting/AnnEconForecast/data/UNRATE.csv',\n",
      " 'b': '/home/ec2-user/AnnEconForecast/data/UNRATE.csv',\n",
      " 'c': '/home/ec2-user/AnnEconForecast/data/DEXCAUS.csv'}\n",
      "Select Dataset >>> b\n",
      "Dataset chosen: /home/ec2-user/AnnEconForecast/data/UNRATE.csv\n",
      "Name of configuration file to load >>> ec2_config\n",
      "Reading configuration file...\n",
      "Loading: dp_config\n"
     ]
    }
   ],
   "source": [
    "# data preparation phase.\n",
    "pprint(constants.DATA_DIR)\n",
    "choice = None\n",
    "while choice is None or choice not in constants.DATA_DIR.keys():\n",
    "    if choice is not None:\n",
    "        print(\"Invalid data location received, try again...\")\n",
    "    choice = input(\"Select Dataset >>> \")\n",
    "FILE_DIR = constants.DATA_DIR[choice]\n",
    "\n",
    "print(f\"Dataset chosen: {FILE_DIR}\")\n",
    "\n",
    "config_name = input(\"Name of configuration file to load >>> \")\n",
    "if config_name.endswith(\".py\"):\n",
    "    config_name = config_name[:-3]\n",
    "\n",
    "exec(f\"import hps_configs.{config_name} as config\")\n",
    "\n",
    "print(\"Reading configuration file...\")\n",
    "for att in dir(config):\n",
    "    if att.endswith(\"_config\"):\n",
    "        print(f\"Loading: {att}\")\n",
    "        exec(f\"globals().update(config.{att})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameter sets generated: 8\n"
     ]
    }
   ],
   "source": [
    "parameter_collection = hps_methods.gen_hparam_set(config.train_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip_grad': None,\n",
      " 'epochs': 100,\n",
      " 'fig_path': '/home/ec2-user/ec2_hps/2018DEC06_01/model_figs/',\n",
      " 'learning_rate': 0.1,\n",
      " 'model_path': '/home/ec2-user/ec2_hps/2018DEC06_01/saved_models/num_time_steps=12-num_neurons=(256, '\n",
      "               '128)-learning_rate=0.1',\n",
      " 'num_inputs': 1,\n",
      " 'num_neurons': (256, 128),\n",
      " 'num_outputs': 1,\n",
      " 'num_time_steps': 12,\n",
      " 'report_periods': 10,\n",
      " 'repr_str': 'num_time_steps=12-num_neurons=(256, 128)-learning_rate=0.1',\n",
      " 'tensorboard_dir': '/home/ec2-user/ec2_hps/2018DEC06_01/tensorboard/num_time_steps=12-num_neurons=(256, '\n",
      "                    '128)-learning_rate=0.1'}\n"
     ]
    }
   ],
   "source": [
    "pprint(parameter_collection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_train(para) -> None:\n",
    "    prepared_df = rnn_prepare.prepare_dataset(\n",
    "        file_dir=FILE_DIR,\n",
    "        periods=PERIODS,\n",
    "        order=ORDER,\n",
    "        remove=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    (X_train, X_val, X_test,\n",
    "     y_train, y_val, y_test) = rnn_prepare.generate_splited_dataset(\n",
    "        raw=prepared_df,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        lags=para[\"num_time_steps\"]\n",
    "    )\n",
    "    data_collection = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "    def checkpoints(z): return [\n",
    "        z*x for x in range(1, para[\"epochs\"] // z)] + [-1]\n",
    "    \n",
    "    (metrics_dict, predictions) = stacked_lstm.exec_core(\n",
    "        parameters=para,\n",
    "        data_collection=data_collection,\n",
    "        prediction_checkpoints=checkpoints(\n",
    "            para[\"epochs\"] // 10\n",
    "        )\n",
    "    )\n",
    "    plt.close()\n",
    "    fig = visualize.plot_checkpoints(predictions, y_test, \"test\")\n",
    "    if not os.path.exists(para[\"fig_path\"]):\n",
    "        os.makedirs(para[\"fig_path\"])\n",
    "    plt.savefig(para[\"fig_path\"] + \"pred_records.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Executing [0/7] hyper-parameter searching session...\n",
      "Dataset loaded.    \n",
      "\tIndex type: datetime64[ns]    \n",
      "\tData type: float64\n",
      "StandardScaler applied, scaling based on the first 679 observations.\n",
      "Total 836 observations generated.\n",
      "Note: shape format: (num_obs, time_steps, num_inputs/outputs)\n",
      "X shape = (836, 12, 1), y shape = (836, 1, 1)\n",
      "Training and testing set generated,        \n",
      "X_train shape: (668, 12, 1)        \n",
      "y_train shape: (668, 1)        \n",
      "X_test shape: (84, 12, 1)        \n",
      "y_test shape: (84, 1)        \n",
      "X_validation shape: (84, 12, 1)        \n",
      "y_validation shape: (84, 1)\n",
      "\n",
      "Iteration [0], Training MSE 20479.1074219; Validation MSE 21411.5332031\n",
      "Saving the trained model...\n",
      "Time taken for [100] epochs:  0:00:03.719635\n",
      "Loss Summary:\n",
      "\tmae=0.5386666059494019\n",
      "\tmse=0.4844203591346741\n",
      "\trmse=0.6960031390190125\n",
      "\tmape=3.9530813694000244\n",
      "Time taken for session [0]: 0:00:05.592822.\n",
      "================================================================\n",
      "Executing [1/7] hyper-parameter searching session...\n",
      "Dataset loaded.    \n",
      "\tIndex type: datetime64[ns]    \n",
      "\tData type: float64\n",
      "StandardScaler applied, scaling based on the first 679 observations.\n",
      "Total 836 observations generated.\n",
      "Note: shape format: (num_obs, time_steps, num_inputs/outputs)\n",
      "X shape = (836, 12, 1), y shape = (836, 1, 1)\n",
      "Training and testing set generated,        \n",
      "X_train shape: (668, 12, 1)        \n",
      "y_train shape: (668, 1)        \n",
      "X_test shape: (84, 12, 1)        \n",
      "y_test shape: (84, 1)        \n",
      "X_validation shape: (84, 12, 1)        \n",
      "y_validation shape: (84, 1)\n",
      "\n",
      "Iteration [0], Training MSE 913.7445679; Validation MSE 1255.7951660\n",
      "Saving the trained model...\n",
      "Time taken for [100] epochs:  0:00:03.969922\n",
      "Loss Summary:\n",
      "\tmae=0.5216137766838074\n",
      "\tmse=0.4525047540664673\n",
      "\trmse=0.6726847290992737\n",
      "\tmape=5.428011417388916\n",
      "Time taken for session [1]: 0:00:05.737889.\n",
      "================================================================\n",
      "Executing [2/7] hyper-parameter searching session...\n",
      "Dataset loaded.    \n",
      "\tIndex type: datetime64[ns]    \n",
      "\tData type: float64\n",
      "StandardScaler applied, scaling based on the first 679 observations.\n",
      "Total 836 observations generated.\n",
      "Note: shape format: (num_obs, time_steps, num_inputs/outputs)\n",
      "X shape = (836, 12, 1), y shape = (836, 1, 1)\n",
      "Training and testing set generated,        \n",
      "X_train shape: (668, 12, 1)        \n",
      "y_train shape: (668, 1)        \n",
      "X_test shape: (84, 12, 1)        \n",
      "y_test shape: (84, 1)        \n",
      "X_validation shape: (84, 12, 1)        \n",
      "y_validation shape: (84, 1)\n",
      "\n",
      "Iteration [0], Training MSE 71088.5781250; Validation MSE 68255.4296875\n",
      "Saving the trained model...\n",
      "Time taken for [100] epochs:  0:00:04.695675\n",
      "Loss Summary:\n",
      "\tmae=1.0063165426254272\n",
      "\tmse=1.8424429893493652\n",
      "\trmse=1.3573662042617798\n",
      "\tmape=25.575634002685547\n",
      "Time taken for session [2]: 0:00:06.593692.\n",
      "================================================================\n",
      "Executing [3/7] hyper-parameter searching session...\n",
      "Dataset loaded.    \n",
      "\tIndex type: datetime64[ns]    \n",
      "\tData type: float64\n",
      "StandardScaler applied, scaling based on the first 679 observations.\n",
      "Total 836 observations generated.\n",
      "Note: shape format: (num_obs, time_steps, num_inputs/outputs)\n",
      "X shape = (836, 12, 1), y shape = (836, 1, 1)\n",
      "Training and testing set generated,        \n",
      "X_train shape: (668, 12, 1)        \n",
      "y_train shape: (668, 1)        \n",
      "X_test shape: (84, 12, 1)        \n",
      "y_test shape: (84, 1)        \n",
      "X_validation shape: (84, 12, 1)        \n",
      "y_validation shape: (84, 1)\n",
      "\n",
      "Iteration [0], Training MSE 32040.9726562; Validation MSE 27867.1835938\n",
      "Saving the trained model...\n",
      "Time taken for [100] epochs:  0:00:04.700822\n",
      "Loss Summary:\n",
      "\tmae=0.59940105676651\n",
      "\tmse=0.5742900967597961\n",
      "\trmse=0.7578192949295044\n",
      "\tmape=7.119978427886963\n",
      "Time taken for session [3]: 0:00:06.647317.\n",
      "================================================================\n",
      "Executing [4/7] hyper-parameter searching session...\n",
      "Dataset loaded.    \n",
      "\tIndex type: datetime64[ns]    \n",
      "\tData type: float64\n",
      "StandardScaler applied, scaling based on the first 679 observations.\n",
      "Total 824 observations generated.\n",
      "Note: shape format: (num_obs, time_steps, num_inputs/outputs)\n",
      "X shape = (824, 24, 1), y shape = (824, 1, 1)\n",
      "Training and testing set generated,        \n",
      "X_train shape: (658, 24, 1)        \n",
      "y_train shape: (658, 1)        \n",
      "X_test shape: (83, 24, 1)        \n",
      "y_test shape: (83, 1)        \n",
      "X_validation shape: (83, 24, 1)        \n",
      "y_validation shape: (83, 1)\n",
      "\n",
      "Iteration [0], Training MSE 61213.8710938; Validation MSE 55772.8359375\n"
     ]
    }
   ],
   "source": [
    "for (i, para) in enumerate(parameter_collection):\n",
    "    print(\"================================================================\")\n",
    "    print(f\"Executing [{i}/{len(parameter_collection) - 1}] hyper-parameter searching session...\")\n",
    "    start = datetime.now()\n",
    "    individual_train(para)\n",
    "    print(f\"Time taken for session [{i}]: {str(datetime.now() - start)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
